{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Audio Classification with Pre-Trained Model\n",
    "\n",
    "This notebook demonstrates feature extraction for our ai model.\n",
    "Feautures we use are:\n",
    "- MFCCs\n",
    "- Chroma\n",
    "- Mel Spectrogram\n",
    "- Spectral Contrast\n",
    "- Tonnetz\n"
   ],
   "id": "61fc92adfae06e34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "We import necessary libraries for audio processing, model handling, and numerical computations.\n",
    "- **TensorFlow/Keras:** Load the pre-trained model\n",
    "- **Joblib:** Load the saved scaler for feature normalization\n",
    "- **Librosa:** Audio feature extraction\n",
    "- **NumPy:** Numerical operations\n",
    "- **JSON:** Handle label mappings"
   ],
   "id": "f2f00473c3311b12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15e747e9257945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## File Paths\n",
    "\n",
    "Define paths to the model, scaler, and label mapping JSON file. Centralized paths simplify maintenance and ensure reproducibility."
   ],
   "id": "c0018b6ca2109bb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70b89936a77752",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/app/song-storage/model.keras\"\n",
    "SCALER_PATH = \"/app/song-storage/scaler.pkl\"\n",
    "LABELS_PATH = \"/app/song-storage/label_order.json\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Model and Scaler\n",
    "\n",
    "Load the trained Keras model and standard scaler.\n",
    "Load the list of dance labels to map predictions back to human-readable classes.\n",
    "This step prepares the environment for inference on new audio files.\n"
   ],
   "id": "6907a518252c606b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c071637d59980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "with open(LABELS_PATH, \"r\") as f:\n",
    "    dance_styles = json.load(f)\n",
    "\n",
    "print(\"Model, scaler, and labels loaded.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Extractor Class\n",
    "\n",
    "`AudioFeatureExtractor` encapsulates all audio processing:\n",
    "\n",
    "- **MFCCs (Mel-Frequency Cepstral Coefficients):** Captures timbral texture\n",
    "- **Chroma Features:** Harmonic content representation\n",
    "- **Mel Spectrogram:** Frequency-energy representation\n",
    "- **Spectral Contrast:** Measures harmonic vs. percussive components\n",
    "- **Tonnetz:** Tonal centroid features for harmony\n",
    "- **Concatenation:** Combines mean and variance of all features into a single feature vector\n",
    "\n",
    "This class also provides a method to directly process an audio file and return a flattened feature vector ready for the model.\n"
   ],
   "id": "28b4591e66d9adb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c53e655b1f2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    def __init__(self, sr=22050, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        y, sr = librosa.load(file_path, sr=self.sr)\n",
    "        y = librosa.util.normalize(y)\n",
    "        return y, sr\n",
    "\n",
    "    def extract_features(self, y):\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=self.n_mfcc, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)\n",
    "        mfccs_var = np.var(mfccs, axis=1)\n",
    "\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        chroma_var = np.var(chroma, axis=1)\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        mel_mean = np.mean(mel, axis=1)\n",
    "        mel_var = np.var(mel, axis=1)\n",
    "\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        contrast_mean = np.mean(contrast, axis=1)\n",
    "        contrast_var = np.var(contrast, axis=1)\n",
    "\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=self.sr)\n",
    "        tonnetz_mean = np.mean(tonnetz, axis=1)\n",
    "        tonnetz_var = np.var(tonnetz, axis=1)\n",
    "\n",
    "        features = np.concatenate([\n",
    "            mfccs_mean, mfccs_var,\n",
    "            chroma_mean, chroma_var,\n",
    "            mel_mean, mel_var,\n",
    "            contrast_mean, contrast_var,\n",
    "            tonnetz_mean, tonnetz_var\n",
    "        ])\n",
    "        return features\n",
    "\n",
    "    def extract_features_from_file(self, file_path):\n",
    "        y, sr = self.load_audio(file_path)\n",
    "        return self.extract_features(y)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification Function\n",
    "\n",
    "`classify_audio` performs the following:\n",
    "\n",
    "1. Extract features from the input audio using `AudioFeatureExtractor`.\n",
    "2. Normalize features using the pre-trained scaler.\n",
    "3. Perform model prediction to obtain probabilities for each dance class.\n",
    "4. Sort predictions in descending order of confidence and return structured output including dance name, confidence score, and speed category.\n"
   ],
   "id": "e09aa8d29f742dde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1cbb2cbf35b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio(file_path, extractor):\n",
    "    features = extractor.extract_features_from_file(file_path).reshape(1, -1)\n",
    "    features = scaler.transform(features)\n",
    "    probabilities = model.predict(features)[0]\n",
    "    predictions = sorted(zip(dance_styles, probabilities), key=lambda x: x[1], reverse=True)\n",
    "    return [\n",
    "        {\"danceName\": dance, \"confidence\": float(f\"{conf:.6f}\"), \"speedCategory\": \"slow\"}\n",
    "        for dance, conf in predictions\n",
    "    ]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example Usage\n",
    "\n",
    "- Instantiate the feature extractor\n",
    "- Provide the path to an audio file\n",
    "- Call `classify_audio` to get predictions\n",
    "\n",
    "The resulting list contains dance names with confidence scores, ready for display or further processing.\n"
   ],
   "id": "e7f3ef45fc100d80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebc38c53f9aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = AudioFeatureExtractor()\n",
    "file_path = \"/path/to/audio/file.wav\"\n",
    "predictions = classify_audio(file_path, extractor)\n",
    "predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
